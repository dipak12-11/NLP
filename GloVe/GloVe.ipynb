{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa976931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4cda81",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86e34b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file_path):\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        text=f.read()\n",
    "    return text\n",
    "def tokenize(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens=text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb31798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='wikipedia.txt'\n",
    "raw_text=read_corpus(file_path)\n",
    "tokens=tokenize(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb0a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts=Counter(tokens)\n",
    "vocab={word:i for i, word in enumerate(word_counts)} #{'apple': 0, 'banana': 1, 'orange': 2}\n",
    "vocab_size=sum([1 for i in dict(word_counts).keys() ])# i like it the cpmlex way...just for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "746250e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "cooccurrence = defaultdict(lambda: defaultdict(float))  # cooccurrence[i][j]\n",
    "\n",
    "for i, word in enumerate(tokens):\n",
    "    word_i = vocab[word]\n",
    "    start = max(i - WINDOW_SIZE, 0)\n",
    "    end = min(i + WINDOW_SIZE + 1, len(tokens))  # FIXED\n",
    "\n",
    "    for j in range(start, end):\n",
    "        if i != j:\n",
    "            word_j = vocab[tokens[j]]\n",
    "            distance = abs(i - j)\n",
    "            cooccurrence[word_i][word_j] += 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f808d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50 \n",
    "W = np.random.randn(vocab_size, embedding_dim) / np.sqrt(embedding_dim)  # word vector\n",
    "W_tilde = np.random.randn(vocab_size, embedding_dim) / np.sqrt(embedding_dim)  # context vector\n",
    "b = np.zeros(vocab_size)           # word bias\n",
    "b_tilde = np.zeros(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6dffd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
